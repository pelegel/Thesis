{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-31T10:44:18.849012Z",
     "start_time": "2024-08-31T10:44:01.744910Z"
    }
   },
   "source": [
    "from sql_connector import mydb, mycursor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', download_dir=r'C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\nltk')\n",
    "nltk.download('stopwords', download_dir=r'C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\nltk')\n",
    "nltk.data.path.append(r'C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\nltk')\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\nltk..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\nltk..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:46:29.111942Z",
     "start_time": "2024-08-31T10:44:28.242051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract columns names\n",
    "mycursor.execute(\"SHOW COLUMNS FROM en_tweets\")\n",
    "tweet_columns = [f\"t.{row[0]}\" for row in mycursor.fetchall()]\n",
    "mycursor.execute(\"SHOW COLUMNS FROM en_users\")\n",
    "user_columns = [f\"u.{row[0]}\" for row in mycursor.fetchall()]\n",
    "mycursor.execute(\"SHOW COLUMNS FROM en_retweets\")\n",
    "retweet_columns = [f\"r.{row[0]}\" for row in mycursor.fetchall()]\n",
    "\n",
    "# Extract tweets with retweets -> Class 1\n",
    "query = f\"\"\"\n",
    "    SELECT {', '.join(tweet_columns + user_columns)}\n",
    "    FROM en_retweets r\n",
    "    JOIN en_users u ON r.original_author_id = u.id\n",
    "    JOIN en_tweets t ON r.original_tweet_id = t.id\n",
    "    WHERE t.created_at <= '2022-12-30 17:00:00'\n",
    "    GROUP BY t.id;\n",
    "\"\"\"\n",
    "mycursor.execute(query)\n",
    "tweets_t = mycursor.fetchall()\n",
    "\n",
    "# Combine the column names for the DataFrame\n",
    "column_names = tweet_columns + user_columns\n",
    "tweets_t = pd.DataFrame(tweets_t, columns=column_names)\n",
    "print(tweets_t.shape)\n",
    "\n",
    "# Extract all tweets\n",
    "query = f\"\"\"\n",
    "    SELECT {', '.join(tweet_columns + user_columns)}\n",
    "    FROM en_tweets t\n",
    "    JOIN en_users u ON t.author_id = u.id\n",
    "    WHERE t.created_at <= '2022-12-30 17:00:00';\n",
    "\"\"\"\n",
    "mycursor.execute(query)\n",
    "tweets_all = mycursor.fetchall()\n",
    "column_names = tweet_columns + user_columns\n",
    "tweets_all = pd.DataFrame(tweets_all, columns=column_names)\n",
    "print(tweets_all.shape)\n",
    "\n",
    "# Extract original tweets that got retweeted\n",
    "mycursor.execute(\"\"\"SELECT original_tweet_id FROM en_retweets WHERE original_tweet_id IS NOT NULL;\"\"\")\n",
    "original_tweets = mycursor.fetchall()\n",
    "column_names = [desc[0] for desc in mycursor.description]\n",
    "original_tweets = pd.DataFrame(original_tweets, columns=column_names)\n",
    "original_tweet_id = set(original_tweets['original_tweet_id'])\n",
    "\n",
    "# Extract tweets with no retweets -> Class 0\n",
    "tweets_no_retweeets = list(set(tweets_all['t.id']) - set(original_tweets['original_tweet_id']))\n",
    "tweets_f = pd.DataFrame(tweets_all[tweets_all['t.id'].isin(tweets_no_retweeets)]) # tweets with no retweets\n",
    "tweets_f.loc[:, 'label'] = 0\n",
    "tweets_t.loc[:, 'label'] = 1\n",
    "\n",
    "print(\"tweets class 1:\", tweets_t.shape)\n",
    "print(\"tweets class 0:\", tweets_f.shape)\n",
    "\n",
    "X_t = pd.concat([tweets_t, tweets_f], axis=0)\n",
    "y_t = X_t['label']\n"
   ],
   "id": "ac8c7244a56e75c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15009, 256)\n",
      "(321061, 256)\n",
      "tweets class 1: (15009, 257)\n",
      "tweets class 0: (306052, 257)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:46:31.422562Z",
     "start_time": "2024-08-31T10:46:29.116930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add week column\n",
    "from datetime import datetime\n",
    "\n",
    "X_t['t.created_at'] = pd.to_datetime(X_t['t.created_at'])\n",
    "\n",
    "def assign_week(date):\n",
    "    if date <= datetime(2022, 12, 9, 17, 0, 0) :\n",
    "        return 1\n",
    "    elif datetime(2022, 12, 9, 17, 0, 0)  <= date <= datetime(2022, 12, 16, 17, 0, 0) :\n",
    "        return 2\n",
    "    elif datetime(2022, 12, 16, 17, 0, 0)  <= date <= datetime(2022, 12, 23, 17, 0, 0) :\n",
    "        return 3\n",
    "    elif datetime(2022, 12, 2, 17, 0, 0)  <= date <= datetime(2022, 12, 30, 17, 0, 0) :\n",
    "        return 4\n",
    "    else:\n",
    "        print(date)\n",
    "\n",
    "# Apply the manual week mappings\n",
    "X_t['week'] = X_t['t.created_at'].apply(assign_week)\n",
    "print(X_t['week'])\n"
   ],
   "id": "9cf52290c4e6a55e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "         ..\n",
      "321056    4\n",
      "321057    4\n",
      "321058    4\n",
      "321059    4\n",
      "321060    4\n",
      "Name: week, Length: 321061, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:49:37.238273Z",
     "start_time": "2024-08-31T10:46:31.424557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing function\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|\\#\\w+', '', text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize (simple split)\n",
    "    tokens = text.split()\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Remove stopwords and specific words, then lemmatize\n",
    "    stop_words = set(stopwords.words('english') + ['chatgpt', 'gpt', 'chat gpt', 'chat'])\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the tweets\n",
    "tweets_all['processed_text'] = tweets_all['t.text'].apply(preprocess_text)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2)\n",
    "tfidf = vectorizer.fit_transform(tweets_all['processed_text'])\n",
    "\n"
   ],
   "id": "a508bbba59c7a967",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:59:53.134651Z",
     "start_time": "2024-08-31T10:49:37.240267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "# Function to get top words for each topic\n",
    "def get_top_words_for_nmf_components(nmf_model, feature_names, num_words=10):\n",
    "    word_dict = {}\n",
    "    for i, topic_vector in enumerate(nmf_model.components_):\n",
    "        top_words_ids = topic_vector.argsort()[-num_words:][::-1]\n",
    "        top_words = [feature_names[id] for id in top_words_ids]\n",
    "        word_dict[f'Topic {i+1}'] = top_words\n",
    "    return word_dict\n",
    "\n",
    "# Function to compute coherence score\n",
    "def compute_coherence_score(nmf_model, tfidf, vectorizer, tweets_all):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_dict = get_top_words_for_nmf_components(nmf_model, feature_names)\n",
    "    \n",
    "    # Prepare documents in a format suitable for Gensim\n",
    "    documents = [[word for word in doc.split() if word in feature_names] for doc in tweets_all['processed_text']]\n",
    "    \n",
    "    # Create a dictionary and corpus for Gensim\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    gensim_corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Train a coherence model\n",
    "    coherence_model = CoherenceModel(topics=[list(word_dict[f'Topic {i+1}']) for i in range(nmf_model.n_components)],\n",
    "                                     texts=documents,\n",
    "                                     dictionary=gensim_dictionary,\n",
    "                                     coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "# Determine optimal number of topics\n",
    "num_topics_range = range(5, 11, 1)  # 5-10 topics\n",
    "coherence_scores = {}\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "    nmf_output = nmf_model.fit_transform(tfidf)\n",
    "    coherence_score = compute_coherence_score(nmf_model, tfidf, vectorizer, tweets_all)\n",
    "    coherence_scores[num_topics] = coherence_score\n",
    "    print(f'Number of Topics: {num_topics}, Coherence Score: {coherence_score}')\n",
    "\n",
    "# Identify the best number of topics\n",
    "best_num_topics = max(coherence_scores, key=coherence_scores.get)\n",
    "print(f'Optimal Number of Topics: {best_num_topics}')\n",
    "\n",
    "# Train NMF with the best number of topics\n",
    "nmf_model = NMF(n_components=best_num_topics, random_state=42)\n",
    "nmf_output = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "# Get the top words for the best topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_dict = get_top_words_for_nmf_components(nmf_model, feature_names)\n",
    "\n",
    "# Print the topics and their top words\n",
    "print(\"\\nTop Words for Each Topic:\")\n",
    "for topic, words in word_dict.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")"
   ],
   "id": "b4ff2aaa045d572",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 5, Coherence Score: 0.5698422065926902\n",
      "Number of Topics: 6, Coherence Score: 0.5616397673309983\n",
      "Number of Topics: 7, Coherence Score: 0.5655517401811604\n",
      "Number of Topics: 8, Coherence Score: 0.5714632351486831\n",
      "Number of Topics: 9, Coherence Score: 0.5728598837312471\n",
      "Number of Topics: 10, Coherence Score: 0.5689931633456018\n",
      "Optimal Number of Topics: 9\n",
      "\n",
      "Top Words for Each Topic:\n",
      "Topic 1: like, think, good, get, make, thing, one, would, time, human\n",
      "Topic 2: google, search, code, red, engine, replace, business, better, result, threat\n",
      "Topic 3: write, asked, poem, code, story, song, style, tweet, essay, result\n",
      "Topic 4: ask, question, let, code, want, someone, tell, anything, maybe, write\n",
      "Topic 5: use, case, way, still, tool, create, person, via, content, free\n",
      "Topic 6: new, via, openai, chatbot, bot, artificial, intelligence, world, article, amp\n",
      "Topic 7: answer, question, give, asked, asking, wrong, stack, correct, overflow, right\n",
      "Topic 8: know, need, even, everything, thing, let, want, well, openai, right\n",
      "Topic 9: using, code, generated, create, generate, help, content, work, start, thread\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:04:11.728456Z",
     "start_time": "2024-08-31T10:59:53.136647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add topic modeling\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', download_dir=r'C:\\\\Users\\\\pelegel\\\\PycharmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data')\n",
    "nltk.download('punkt_tab', download_dir=r'C:\\\\Users\\\\pelegel\\\\PycharmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "    text = re.sub(r'@\\w+|\\#\\w+', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)  # Remove special characters and digits\n",
    "    text = text.lower()    # Convert to lowercase\n",
    "    tokens = word_tokenize(text)   # Tokenize\n",
    "    lemmatizer = WordNetLemmatizer()    # Initialize lemmatizer\n",
    "    # Remove stopwords and specific words, then lemmatize\n",
    "    stop_words = set(stopwords.words('english') + ['chatgpt', 'gpt', 'chat gpt', 'chat'])\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the tweets\n",
    "X_t['processed_text'] = X_t['t.text'].apply(preprocess_text)\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2)\n",
    "tfidf = vectorizer.fit_transform(X_t['processed_text'])\n",
    "\n",
    "# Apply NMF\n",
    "num_topics = best_num_topics\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "nmf_output = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "# Get the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_dict = {}\n",
    "for i in range(num_topics):\n",
    "    words_ids = nmf_model.components_[i].argsort()[:-10 - 1:-1]\n",
    "    words = [feature_names[key] for key in words_ids]\n",
    "    word_dict[f'Topic {i+1}'] = words\n",
    "\n",
    "# Print the topics and their top words\n",
    "for topic, words in word_dict.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")\n",
    "\n",
    "# Assign topics to tweets\n",
    "X_t['dominant_topic'] = nmf_output.argmax(axis=1)\n",
    "\n",
    "# Print a few examples\n",
    "print(\"\\nExample tweets and their dominant topics:\")\n",
    "for i in range(5):  # Print first 5 examples\n",
    "    tweet = X_t['t.text'].iloc[i]\n",
    "    topic = X_t['dominant_topic'].iloc[i]\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Dominant Topic: Topic {topic + 1}\")\n",
    "    print(f\"Top words: {', '.join(word_dict[f'Topic {topic + 1}'][:5])}\\n\")\n",
    "\n",
    "print(\"All Distinct Topics:\")\n",
    "for topic, words in word_dict.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")\n",
    "\n",
    "print(X_t['dominant_topic'].value_counts())\n",
    "X_t['dominant_topic'] = X_t['dominant_topic'] + 1\n",
    "one_hot_encoded = pd.get_dummies(X_t['dominant_topic'], prefix='topic')\n",
    "X_t = pd.concat([X_t, one_hot_encoded], axis=1)"
   ],
   "id": "a0defe8136685a17",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\\\Users\\\\pelegel\\\\PycharmPr\n",
      "[nltk_data]     ojects\\\\pythonProject1\\\\myenv\\\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\\\Users\\\\pelegel\\\\Pycha\n",
      "[nltk_data]     rmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: like, think, good, get, make, thing, one, would, time, human\n",
      "Topic 2: google, search, code, red, engine, replace, business, better, result, threat\n",
      "Topic 3: write, asked, poem, code, story, song, style, tweet, essay, got\n",
      "Topic 4: ask, question, let, code, want, someone, tell, anything, maybe, get\n",
      "Topic 5: use, case, way, still, tool, create, person, via, content, free\n",
      "Topic 6: new, via, openai, chatbot, bot, artificial, intelligence, world, article, amp\n",
      "Topic 7: answer, question, give, asked, asking, wrong, stack, correct, overflow, right\n",
      "Topic 8: know, need, even, everything, thing, let, want, well, openai, right\n",
      "Topic 9: using, code, generated, create, generate, help, content, work, start, thread\n",
      "\n",
      "Example tweets and their dominant topics:\n",
      "Tweet: I asked ChatGPT what it thinks about Mutiny Web https://t.co/dOi1pFNQ16\n",
      "Dominant Topic: Topic 3\n",
      "Top words: write, asked, poem, code, story\n",
      "\n",
      "Tweet: People tricking ChatGPT ‚Äúlike watching an Asimov novel come to life‚Äù https://t.co/DKc4CT320n\n",
      "Dominant Topic: Topic 1\n",
      "Top words: like, think, good, get, make\n",
      "\n",
      "Tweet: Phishing/scam emails are about to get a lot better. #ChatGPT\n",
      "Dominant Topic: Topic 1\n",
      "Top words: like, think, good, get, make\n",
      "\n",
      "Tweet: This was written using ChatGPT https://t.co/0hFNTjf2uf\n",
      "Dominant Topic: Topic 9\n",
      "Top words: using, code, generated, create, generate\n",
      "\n",
      "Tweet: ChatGPT decompiling assembly is pretty impressive. Watch out @HexRaysSA üëÄ https://t.co/zkEk9qIyxO\n",
      "Dominant Topic: Topic 1\n",
      "Top words: like, think, good, get, make\n",
      "\n",
      "All Distinct Topics:\n",
      "Topic 1: like, think, good, get, make, thing, one, would, time, human\n",
      "Topic 2: google, search, code, red, engine, replace, business, better, result, threat\n",
      "Topic 3: write, asked, poem, code, story, song, style, tweet, essay, got\n",
      "Topic 4: ask, question, let, code, want, someone, tell, anything, maybe, get\n",
      "Topic 5: use, case, way, still, tool, create, person, via, content, free\n",
      "Topic 6: new, via, openai, chatbot, bot, artificial, intelligence, world, article, amp\n",
      "Topic 7: answer, question, give, asked, asking, wrong, stack, correct, overflow, right\n",
      "Topic 8: know, need, even, everything, thing, let, want, well, openai, right\n",
      "Topic 9: using, code, generated, create, generate, help, content, work, start, thread\n",
      "dominant_topic\n",
      "0    158713\n",
      "5     38002\n",
      "2     26569\n",
      "8     21801\n",
      "6     21030\n",
      "1     16813\n",
      "4     15054\n",
      "7     14239\n",
      "3      8840\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:04:18.254001Z",
     "start_time": "2024-08-31T11:04:11.729501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove retweets columns\n",
    "X_t = X_t.drop(columns=[col for col in X_t.columns if col.startswith('r.')])\n",
    "X_t = X_t.drop(columns=['t.id', 't.edit_history_tweet_ids', 't.created_at', 't.lang', 't.text', 't.author_id', 't.screen_name',\n",
    "                        't.entities', 't.geo', 't.withheld', 't.created_at', 't.in_reply_to_user_id', 't.retweet_count', 't.reply_count',\n",
    "                        't.like_count', 't.quote_count', 't.impression_count', 't.original_author_screen_name',\n",
    "                        'u.id', 'u.id_str', 'u.name', 'u.screen_name', 'u.location', 'u.profile_location', 'u.description', 'u.entities',\n",
    "                        'u.created_at', 'u.utc_offset', 'u.time_zone', 'u.lang', 'u.status', 'u.translator_type',\n",
    "                        'u.withheld_in_countries',\n",
    "                        'u.a1_in_degree', 'u.b1_in_degree', 'u.c1_in_degree',\n",
    "                        'u.a2_in_degree', 'u.b2_in_degree', 'u.c2_in_degree',\n",
    "                        'u.a3_in_degree', 'u.b3_in_degree', 'u.c3_in_degree',\n",
    "                        'u.a4_in_degree', 'u.b4_in_degree', 'u.c4_in_degree',\n",
    "                        'u.a1_closeness', 'u.b1_closeness', 'u.c1_closeness',\n",
    "                        'u.a2_closeness', 'u.b2_closeness', 'u.c2_closeness',\n",
    "                        'u.a3_closeness', 'u.b3_closeness', 'u.c3_closeness',\n",
    "                        'u.a4_closeness', 'u.b4_closeness', 'u.c4_closeness', 't.week', 'processed_text', 'dominant_topic',\n",
    "                        'label'])\n",
    "\n",
    "X_t.to_parquet('X_t_tm_new.parquet', index=False)\n",
    "y_t_df = pd.DataFrame(y_t)\n",
    "y_t_df.to_parquet('y_t_tm_new.parquet', index=False)\n",
    "\n",
    "print(X_t.columns.to_list())\n",
    "\n",
    "correlations = X_t.corrwith(y_t)\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "print(correlations.head(30))"
   ],
   "id": "ebbf60fce8ee21b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t.hashtags', 't.urls', 't.user_mentions', 't.admiration', 't.amusement', 't.anger', 't.annoyance', 't.approval', 't.caring', 't.confusion', 't.curiosity', 't.desire', 't.disappointment', 't.disapproval', 't.disgust', 't.embarrassment', 't.excitement', 't.fear', 't.gratitude', 't.grief', 't.joy', 't.love', 't.nervousness', 't.neutral', 't.optimism', 't.pride', 't.realization', 't.relief', 't.remorse', 't.sadness', 't.surprise', 't.seniority_at_tweeting', 'u.protected', 'u.followers_count', 'u.friends_count', 'u.listed_count', 'u.favourites_count', 'u.geo_enabled', 'u.verified', 'u.statuses_count', 'u.contributors_enabled', 'u.is_translator', 'u.is_translation_enabled', 'u.has_extended_profile', 'u.default_profile', 'u.default_profile_image', 'u.following', 'u.follow_request_sent', 'u.notifications', 'u.retweets_num', 'u.admiration_avg', 'u.admiration_min', 'u.admiration_max', 'u.admiration_std', 'u.amusement_avg', 'u.amusement_min', 'u.amusement_max', 'u.amusement_std', 'u.anger_avg', 'u.anger_min', 'u.anger_max', 'u.anger_std', 'u.annoyance_avg', 'u.annoyance_min', 'u.annoyance_max', 'u.annoyance_std', 'u.approval_avg', 'u.approval_min', 'u.approval_max', 'u.approval_std', 'u.caring_avg', 'u.caring_min', 'u.caring_max', 'u.caring_std', 'u.confusion_avg', 'u.confusion_min', 'u.confusion_max', 'u.confusion_std', 'u.curiosity_avg', 'u.curiosity_min', 'u.curiosity_max', 'u.curiosity_std', 'u.desire_avg', 'u.desire_min', 'u.desire_max', 'u.desire_std', 'u.disappointment_avg', 'u.disappointment_min', 'u.disappointment_max', 'u.disappointment_std', 'u.disapproval_avg', 'u.disapproval_min', 'u.disapproval_max', 'u.disapproval_std', 'u.disgust_avg', 'u.disgust_min', 'u.disgust_max', 'u.disgust_std', 'u.embarrassment_avg', 'u.embarrassment_min', 'u.embarrassment_max', 'u.embarrassment_std', 'u.excitement_avg', 'u.excitement_min', 'u.excitement_max', 'u.excitement_std', 'u.fear_avg', 'u.fear_min', 'u.fear_max', 'u.fear_std', 'u.gratitude_avg', 'u.gratitude_min', 'u.gratitude_max', 'u.gratitude_std', 'u.grief_avg', 'u.grief_min', 'u.grief_max', 'u.grief_std', 'u.joy_avg', 'u.joy_min', 'u.joy_max', 'u.joy_std', 'u.love_avg', 'u.love_min', 'u.love_max', 'u.love_std', 'u.nervousness_avg', 'u.nervousness_min', 'u.nervousness_max', 'u.nervousness_std', 'u.neutral_avg', 'u.neutral_min', 'u.neutral_max', 'u.neutral_std', 'u.optimism_avg', 'u.optimism_min', 'u.optimism_max', 'u.optimism_std', 'u.pride_avg', 'u.pride_min', 'u.pride_max', 'u.pride_std', 'u.realization_avg', 'u.realization_min', 'u.realization_max', 'u.realization_std', 'u.relief_avg', 'u.relief_min', 'u.relief_max', 'u.relief_std', 'u.remorse_avg', 'u.remorse_min', 'u.remorse_max', 'u.remorse_std', 'u.sadness_avg', 'u.sadness_min', 'u.sadness_max', 'u.sadness_std', 'u.surprise_avg', 'u.surprise_min', 'u.surprise_max', 'u.surprise_std', 'u.is_in_a1', 'u.a1_out_degree', 'u.a1_betweenness', 'u.is_in_b1', 'u.b1_out_degree', 'u.b1_betweenness', 'u.is_in_c1', 'u.c1_out_degree', 'u.c1_betweenness', 'u.is_in_a2', 'u.a2_out_degree', 'u.a2_betweenness', 'u.is_in_b2', 'u.b2_out_degree', 'u.b2_betweenness', 'u.is_in_c2', 'u.c2_out_degree', 'u.c2_betweenness', 'u.is_in_a3', 'u.a3_out_degree', 'u.a3_betweenness', 'u.is_in_b3', 'u.b3_out_degree', 'u.b3_betweenness', 'u.is_in_c3', 'u.c3_out_degree', 'u.c3_betweenness', 'u.is_in_a4', 'u.a4_out_degree', 'u.a4_betweenness', 'u.is_in_b4', 'u.b4_out_degree', 'u.b4_betweenness', 'u.is_in_c4', 'u.c4_out_degree', 'u.c4_betweenness', 'u.seniority', 'week', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u.listed_count              0.073145\n",
      "t.user_mentions             0.062612\n",
      "u.favourites_count          0.051370\n",
      "t.urls                      0.049193\n",
      "u.default_profile           0.040981\n",
      "u.followers_count           0.040612\n",
      "t.seniority_at_tweeting     0.038330\n",
      "u.seniority                 0.038330\n",
      "u.friends_count             0.036982\n",
      "u.a3_betweenness            0.036166\n",
      "u.geo_enabled               0.032121\n",
      "u.statuses_count            0.031495\n",
      "u.a1_betweenness            0.028137\n",
      "t.approval                  0.028038\n",
      "t.neutral                   0.025638\n",
      "t.realization               0.023934\n",
      "u.curiosity_avg             0.022429\n",
      "u.confusion_avg             0.021222\n",
      "u.curiosity_std             0.021106\n",
      "week                        0.020647\n",
      "t.optimism                  0.019993\n",
      "u.confusion_std             0.019686\n",
      "u.a4_out_degree             0.018608\n",
      "u.realization_min           0.018544\n",
      "u.curiosity_max             0.018475\n",
      "u.is_translation_enabled    0.018379\n",
      "u.b1_betweenness            0.018291\n",
      "u.confusion_max             0.018210\n",
      "u.b4_out_degree             0.017604\n",
      "topic_5                     0.016873\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
