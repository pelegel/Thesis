{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-28T13:07:05.663051Z",
     "start_time": "2024-08-28T13:07:03.941765Z"
    }
   },
   "source": [
    "from sql_connector import mydb, mycursor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:07:32.422355Z",
     "start_time": "2024-08-28T13:07:05.665044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract all retweets\n",
    "mycursor.execute(\"\"\"select *\n",
    "                    FROM en_retweets\n",
    "                    where original_tweet_text is not null\n",
    "                    \"\"\")\n",
    "retweets_all = mycursor.fetchall()\n",
    "retweets_all = pd.DataFrame(retweets_all, columns=[desc[0] for desc in mycursor.description])\n",
    "print(\"all retweets: \", retweets_all.shape)"
   ],
   "id": "30a12913eb6f60d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all retweets:  (448519, 53)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:07:37.180976Z",
     "start_time": "2024-08-28T13:07:32.428320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add topic modeling\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', download_dir=r'C:\\\\Users\\\\pelegel\\\\PycharmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data')\n",
    "nltk.download('punkt_tab', download_dir=r'C:\\\\Users\\\\pelegel\\\\PycharmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "    text = re.sub(r'@\\w+|\\#\\w+', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)  # Remove special characters and digits\n",
    "    text = text.lower()    # Convert to lowercase\n",
    "    tokens = word_tokenize(text)   # Tokenize\n",
    "    lemmatizer = WordNetLemmatizer()    # Initialize lemmatizer\n",
    "    # Remove stopwords and specific words, then lemmatize\n",
    "    stop_words = set(stopwords.words('english') + ['chatgpt', 'gpt', 'chat gpt', 'chat'])\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ],
   "id": "ef98de598a1a8dd4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\\\Users\\\\pelegel\\\\PycharmPr\n",
      "[nltk_data]     ojects\\\\pythonProject1\\\\myenv\\\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\\\Users\\\\pelegel\\\\Pycha\n",
      "[nltk_data]     rmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:14:03.332934Z",
     "start_time": "2024-08-28T13:07:37.185954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add topic modeling\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', download_dir=r'C:\\\\Users\\\\pelegel\\\\PycharmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data')\n",
    "nltk.download('punkt_tab', download_dir=r'C:\\\\Users\\\\pelegel\\\\PycharmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data')\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "retweets_all['processed_text'] = retweets_all['original_tweet_text'].apply(preprocess_text)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2)\n",
    "tfidf = vectorizer.fit_transform(retweets_all['processed_text'])\n",
    "\n",
    "# Apply NMF\n",
    "num_topics = 9\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "nmf_output = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "# Get the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_dict = {}\n",
    "for i in range(num_topics):\n",
    "    words_ids = nmf_model.components_[i].argsort()[:-10 - 1:-1]\n",
    "    words = [feature_names[key] for key in words_ids]\n",
    "    word_dict[f'Topic {i+1}'] = words\n",
    "\n",
    "# Print the topics and their top words\n",
    "for topic, words in word_dict.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")\n",
    "\n",
    "# Assign topics to tweets\n",
    "retweets_all['dominant_topic'] = nmf_output.argmax(axis=1)\n",
    "retweets_all['dominant_topic'] = retweets_all['dominant_topic'] + 1\n",
    "print(retweets_all['dominant_topic'])\n",
    "one_hot_encoded = pd.get_dummies(retweets_all['dominant_topic'], prefix='topic')\n",
    "retweets_all = pd.concat([retweets_all, one_hot_encoded], axis=1)"
   ],
   "id": "d76649a871fa36d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\\\Users\\\\pelegel\\\\PycharmPr\n",
      "[nltk_data]     ojects\\\\pythonProject1\\\\myenv\\\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\\\Users\\\\pelegel\\\\Pycha\n",
      "[nltk_data]     rmProjects\\\\pythonProject1\\\\myenv\\\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: month, user, netflix, took, instagram, crossed, compare, day, year, million\n",
      "Topic 2: thought, architecture, solve, implemented, blockchain, unique, open, asked, write, answer\n",
      "Topic 3: use, google, amp, guide, full, created, bring, potential, ad, figure\n",
      "Topic 4: one, ago, whisper, tool, copilot, codex, github, instructgpt, exist, slide\n",
      "Topic 5: done, nothing, week, senior, pov, asks, frantically, engineer, elon, data\n",
      "Topic 6: today, game, thing, real, want, system, share, please, changing, thr\n",
      "Topic 7: new, far, left, york, justice, cause, social, rename, time, good\n",
      "Topic 8: offer, fossil, fuel, argument, expressly, prohibits, alarm, used, prohibited, promoting\n",
      "Topic 9: email, addy, write, introducing, powered, assistant, done, asked, story, using\n",
      "0         3\n",
      "1         3\n",
      "2         3\n",
      "3         9\n",
      "4         3\n",
      "         ..\n",
      "448514    6\n",
      "448515    1\n",
      "448516    6\n",
      "448517    2\n",
      "448518    6\n",
      "Name: dominant_topic, Length: 448519, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:06:37.197975Z",
     "start_time": "2024-08-28T13:06:37.094131Z"
    }
   },
   "cell_type": "code",
   "source": "retweets_all = retweets_all.drop(['dominant_topic', 'processed_text'], axis=1)",
   "id": "3c5c270703a6b2e7",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['dominant_topic', 'processed_text'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m retweets_all \u001B[38;5;241m=\u001B[39m \u001B[43mretweets_all\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdominant_topic\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprocessed_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py:5568\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   5420\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[0;32m   5421\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5422\u001B[0m     labels: IndexLabel \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5429\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   5430\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   5431\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   5432\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[0;32m   5433\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5566\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[0;32m   5567\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 5568\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   5569\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5570\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5571\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5572\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5573\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5574\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5575\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5576\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py:4785\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   4783\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   4784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 4785\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4787\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[0;32m   4788\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py:4827\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[1;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[0;32m   4825\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m   4826\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 4827\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4828\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[0;32m   4830\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[0;32m   4831\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001B[0m, in \u001B[0;36mIndex.drop\u001B[1;34m(self, labels, errors)\u001B[0m\n\u001B[0;32m   7068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m   7069\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 7070\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels[mask]\u001B[38;5;241m.\u001B[39mtolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   7071\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[0;32m   7072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['dominant_topic', 'processed_text'] not found in axis\""
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:16:53.226455Z",
     "start_time": "2024-08-28T13:14:03.335919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract users who first retweeted and then tweeted -> Class 1\n",
    "mycursor.execute(\"\"\"\n",
    "                    select u.*\n",
    "                    FROM en_users u\n",
    "                    JOIN (SELECT author_id, MIN(created_at) AS earliest_retweet FROM en_retweets GROUP BY author_id) r ON u.id = r.author_id\n",
    "                    JOIN (SELECT author_id, MIN(created_at) AS earliest_tweet FROM en_tweets GROUP BY author_id) t ON u.id = t.author_id\n",
    "                    WHERE t.earliest_tweet > r.earliest_retweet;\n",
    "                    \"\"\")\n",
    "users_t = mycursor.fetchall()\n",
    "users_t = pd.DataFrame(users_t, columns=[desc[0] for desc in mycursor.description])\n",
    "print(\"users class 1 :\", users_t.shape)\n",
    "\n",
    "# Extract users who retweeted but didn't tweeted at all -> Class 0\n",
    "mycursor.execute(\"\"\"\n",
    "                    SELECT u.*\n",
    "                    FROM en_retweets r join en_users u on r.author_id=u.id\n",
    "                    WHERE author_id NOT IN (SELECT author_id FROM en_tweets) AND author_id IS NOT NULL\n",
    "                    GROUP BY author_id;\n",
    "                    \"\"\")\n",
    "users_f = mycursor.fetchall()\n",
    "users_f = pd.DataFrame(users_f, columns=[desc[0] for desc in mycursor.description])\n",
    "print(\"users class 0:\", users_f.shape)\n",
    "\n",
    "# Adding target variable\n",
    "users_t['label'] = 1\n",
    "users_f['label'] = 0\n",
    "\n",
    "# Concat samples\n",
    "samples = pd.concat([users_t, users_f], axis=0)\n",
    "samples= samples.fillna(0)\n",
    "\n",
    "\n"
   ],
   "id": "21b71bad1da2085",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users class 1 : (15892, 206)\n",
      "users class 0: (232488, 206)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pelegel\\AppData\\Local\\Temp\\ipykernel_18140\\732016161.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  samples = pd.concat([users_t, users_f], axis=0)\n",
      "C:\\Users\\pelegel\\AppData\\Local\\Temp\\ipykernel_18140\\732016161.py:30: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  samples= samples.fillna(0)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:16:55.526801Z",
     "start_time": "2024-08-28T13:16:53.228450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retweets_all = retweets_all[['author_id', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9']]\n",
    "\n",
    "grouped_retweets = retweets_all.groupby('author_id').sum().reset_index()\n",
    "samples = pd.merge(samples, grouped_retweets, left_on='id', right_on='author_id', how='left')\n",
    "samples.drop('author_id', axis=1, inplace=True)\n",
    "print(samples)\n"
   ],
   "id": "e9092a7ba53b4022",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id               id_str                   name  \\\n",
      "0                       324                  324           Chris Fralic   \n",
      "1                       339                  339              Tony Rose   \n",
      "2                       544                  544                  Peter   \n",
      "3                       765                  765        Sean Bonner Ⓥ🛡️   \n",
      "4                      1497                 1497               harper 🤯   \n",
      "...                     ...                  ...                    ...   \n",
      "248375  1609884823602212865  1609884823602212865            Tarun_Kumar   \n",
      "248376  1609909861898227714  1609909861898227714  Miguel González-Bueno   \n",
      "248377  1609911768519970817  1609911768519970817              Eve Hough   \n",
      "248378  1609915175330537474  1609915175330537474              Sweetness   \n",
      "248379  1609917257383124996  1609917257383124996         Simple n Sweet   \n",
      "\n",
      "            screen_name           location  \\\n",
      "0           chrisfralic      Phillydelphia   \n",
      "1           tonyrose023                      \n",
      "2                 peter      San Francisco   \n",
      "3            seanbonner  Vancouver / Tokyo   \n",
      "4                harper  harper@modest.com   \n",
      "...                 ...                ...   \n",
      "248375   chottuthejimmy          Bengaluru   \n",
      "248376       Miguelgbe3                      \n",
      "248377         EveHough          Australia   \n",
      "248378  Sweetne12394869                      \n",
      "248379    simplensweetb                      \n",
      "\n",
      "                                         profile_location  \\\n",
      "0                                                    None   \n",
      "1                                                    None   \n",
      "2                                                    None   \n",
      "3                                                    None   \n",
      "4                                                    None   \n",
      "...                                                   ...   \n",
      "248375                                               None   \n",
      "248376                                               None   \n",
      "248377  {'id': '3f14ce28dc7c4566', 'url': 'https://api...   \n",
      "248378                                               None   \n",
      "248379                                               None   \n",
      "\n",
      "                                              description  \\\n",
      "0       Board Partner @FirstRound. Technology historia...   \n",
      "1       Head of Product, Atala at IOG, https://t.co/da...   \n",
      "2       Partner @m12vc, investing in B2B SaaS, fintech...   \n",
      "3       Misanthropologist. Professional exit liquidity...   \n",
      "4       A normal person doing normal things. I take ph...   \n",
      "...                                                   ...   \n",
      "248375                                       DETACHMENT!!   \n",
      "248376                                                      \n",
      "248377  Passionate about helping education businesses ...   \n",
      "248378                                                      \n",
      "248379                                                      \n",
      "\n",
      "                                                 entities  protected  \\\n",
      "0       {'url': {'urls': [{'url': 'https://t.co/wc5VRc...          0   \n",
      "1       {'description': {'urls': [{'url': 'https://t.c...          0   \n",
      "2       {'url': {'urls': [{'url': 'https://t.co/amjPvW...          0   \n",
      "3       {'url': {'urls': [{'url': 'https://t.co/2Wk5WN...          0   \n",
      "4       {'url': {'urls': [{'url': 'https://t.co/BdjGHr...          0   \n",
      "...                                                   ...        ...   \n",
      "248375                      {'description': {'urls': []}}          0   \n",
      "248376                      {'description': {'urls': []}}          0   \n",
      "248377  {'url': {'urls': [{'url': 'https://t.co/HvDvwB...          0   \n",
      "248378                      {'description': {'urls': []}}          0   \n",
      "248379                      {'description': {'urls': []}}          0   \n",
      "\n",
      "        followers_count  ...  label  topic_1  topic_2 topic_3 topic_4 topic_5  \\\n",
      "0                 41204  ...      1        1        0       1       0       0   \n",
      "1                  1671  ...      1        0        0       2       0       0   \n",
      "2                 16807  ...      1        1        0       0       0       0   \n",
      "3                 25126  ...      1        0        0       1       0       0   \n",
      "4                 37169  ...      1        0        0       4       1       0   \n",
      "...                 ...  ...    ...      ...      ...     ...     ...     ...   \n",
      "248375               10  ...      0        0        0       0       0       0   \n",
      "248376                0  ...      0        0        0       1       0       0   \n",
      "248377                3  ...      0        0        0       0       0       0   \n",
      "248378                1  ...      0        1        0       0       0       0   \n",
      "248379                1  ...      0        1        0       0       0       0   \n",
      "\n",
      "        topic_6  topic_7  topic_8 topic_9  \n",
      "0             1        1        0       0  \n",
      "1             1        0        0       0  \n",
      "2             1        0        0       0  \n",
      "3             1        0        0       1  \n",
      "4             0        1        0       2  \n",
      "...         ...      ...      ...     ...  \n",
      "248375        0        0        0       1  \n",
      "248376        0        0        0       0  \n",
      "248377        1        0        0       0  \n",
      "248378        0        0        0       0  \n",
      "248379        0        0        0       0  \n",
      "\n",
      "[248380 rows x 216 columns]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:16:57.222457Z",
     "start_time": "2024-08-28T13:16:55.528788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "cols_to_drop = (['id', 'id_str', 'name', 'screen_name', 'location', 'profile_location', 'description', 'entities',\n",
    "                'created_at', 'utc_offset', 'time_zone', 'lang', 'status', 'translator_type',\n",
    "                'withheld_in_countries', 'retweets_num'\n",
    "                 , 'c4_closeness',\n",
    "                 'a1_in_degree', 'b1_in_degree', 'c1_in_degree',\n",
    "                 'a2_in_degree', 'b2_in_degree', 'c2_in_degree',\n",
    "                 'a3_in_degree', 'b3_in_degree', 'c3_in_degree',\n",
    "                 'a4_in_degree', 'b4_in_degree', 'c4_in_degree',\n",
    "                 'is_in_a1', 'is_in_a2', 'is_in_a3', 'is_in_a4',\n",
    "                 'is_in_b1', 'is_in_b2', 'is_in_b3', 'is_in_b4',\n",
    "                 'is_in_c1', 'is_in_c2', 'is_in_c3', 'is_in_c4'])\n",
    "\n",
    "\n",
    "samples = samples.drop(columns=cols_to_drop)\n",
    "y_u = samples['label']\n",
    "X_u = samples.drop(columns=['label'])\n",
    "\n",
    "correlations = X_u.corrwith(y_u)\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "print(correlations.head(30))"
   ],
   "id": "ac5c683a6d1f2cfb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\pelegel\\PycharmProjects\\pythonProject1\\myenv\\Lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral_std           0.192784\n",
      "approval_std          0.155188\n",
      "admiration_std        0.127515\n",
      "annoyance_std         0.119979\n",
      "curiosity_std         0.119149\n",
      "disapproval_std       0.118155\n",
      "realization_std       0.116055\n",
      "disappointment_std    0.113753\n",
      "neutral_min           0.110237\n",
      "excitement_std        0.109448\n",
      "relief_std            0.104936\n",
      "confusion_std         0.099803\n",
      "a1_closeness          0.098940\n",
      "admiration_max        0.098713\n",
      "a1_out_degree         0.097856\n",
      "disapproval_max       0.096498\n",
      "joy_std               0.095871\n",
      "annoyance_max         0.095415\n",
      "disappointment_max    0.092698\n",
      "grief_std             0.089106\n",
      "pride_std             0.088683\n",
      "approval_max          0.087618\n",
      "curiosity_max         0.085564\n",
      "topic_6               0.085416\n",
      "listed_count          0.085295\n",
      "topic_9               0.084355\n",
      "topic_3               0.082914\n",
      "topic_5               0.082639\n",
      "confusion_max         0.078384\n",
      "amusement_std         0.078035\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T09:21:06.442287Z",
     "start_time": "2024-08-27T09:21:06.435951Z"
    }
   },
   "cell_type": "code",
   "source": "print(X_u.columns.to_list())",
   "id": "5f36293b30494048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['protected', 'followers_count', 'friends_count', 'listed_count', 'favourites_count', 'geo_enabled', 'verified', 'statuses_count', 'contributors_enabled', 'is_translator', 'is_translation_enabled', 'has_extended_profile', 'default_profile', 'default_profile_image', 'following', 'follow_request_sent', 'notifications', 'admiration_avg', 'admiration_min', 'admiration_max', 'admiration_std', 'amusement_avg', 'amusement_min', 'amusement_max', 'amusement_std', 'anger_avg', 'anger_min', 'anger_max', 'anger_std', 'annoyance_avg', 'annoyance_min', 'annoyance_max', 'annoyance_std', 'approval_avg', 'approval_min', 'approval_max', 'approval_std', 'caring_avg', 'caring_min', 'caring_max', 'caring_std', 'confusion_avg', 'confusion_min', 'confusion_max', 'confusion_std', 'curiosity_avg', 'curiosity_min', 'curiosity_max', 'curiosity_std', 'desire_avg', 'desire_min', 'desire_max', 'desire_std', 'disappointment_avg', 'disappointment_min', 'disappointment_max', 'disappointment_std', 'disapproval_avg', 'disapproval_min', 'disapproval_max', 'disapproval_std', 'disgust_avg', 'disgust_min', 'disgust_max', 'disgust_std', 'embarrassment_avg', 'embarrassment_min', 'embarrassment_max', 'embarrassment_std', 'excitement_avg', 'excitement_min', 'excitement_max', 'excitement_std', 'fear_avg', 'fear_min', 'fear_max', 'fear_std', 'gratitude_avg', 'gratitude_min', 'gratitude_max', 'gratitude_std', 'grief_avg', 'grief_min', 'grief_max', 'grief_std', 'joy_avg', 'joy_min', 'joy_max', 'joy_std', 'love_avg', 'love_min', 'love_max', 'love_std', 'nervousness_avg', 'nervousness_min', 'nervousness_max', 'nervousness_std', 'neutral_avg', 'neutral_min', 'neutral_max', 'neutral_std', 'optimism_avg', 'optimism_min', 'optimism_max', 'optimism_std', 'pride_avg', 'pride_min', 'pride_max', 'pride_std', 'realization_avg', 'realization_min', 'realization_max', 'realization_std', 'relief_avg', 'relief_min', 'relief_max', 'relief_std', 'remorse_avg', 'remorse_min', 'remorse_max', 'remorse_std', 'sadness_avg', 'sadness_min', 'sadness_max', 'sadness_std', 'surprise_avg', 'surprise_min', 'surprise_max', 'surprise_std', 'a1_out_degree', 'a1_betweenness', 'a1_closeness', 'b1_out_degree', 'b1_betweenness', 'b1_closeness', 'c1_out_degree', 'c1_betweenness', 'c1_closeness', 'a2_out_degree', 'a2_betweenness', 'a2_closeness', 'b2_out_degree', 'b2_betweenness', 'b2_closeness', 'c2_out_degree', 'c2_betweenness', 'c2_closeness', 'a3_out_degree', 'a3_betweenness', 'a3_closeness', 'b3_out_degree', 'b3_betweenness', 'b3_closeness', 'c3_out_degree', 'c3_betweenness', 'c3_closeness', 'a4_out_degree', 'a4_betweenness', 'a4_closeness', 'b4_out_degree', 'b4_betweenness', 'b4_closeness', 'c4_out_degree', 'c4_betweenness', 'seniority', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:16:59.743692Z",
     "start_time": "2024-08-28T13:16:57.224456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_u.to_parquet('X_u_tm_1.parquet', index=False)\n",
    "y_u_df = pd.DataFrame(y_u)\n",
    "y_u_df.to_parquet('y_u_tm_1.parquet', index=False)"
   ],
   "id": "2c5ccce9989471b3",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
